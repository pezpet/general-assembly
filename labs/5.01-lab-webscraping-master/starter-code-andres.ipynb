{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping lab\n",
    "\n",
    "Practice your webscraping and parsing skills! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://pages.git.generalassemb.ly/rldaggie/for-scraping/'\n",
    "res = requests.get(url)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and path from the `href`. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'href': 'restaurants/1.html'}, \n",
    "    {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = []\n",
    "rows = soup.find_all('td')\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['name'] = row.find('a').text\n",
    "    row_dict['href'] = row.find('a')['href']\n",
    "    restaurants.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'A&W Restaurants', 'href': 'restaurants/1.html'},\n",
       " {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
       " {'name': \"Arby's\", 'href': 'restaurants/3.html'},\n",
       " {'name': 'Atlanta Bread Company', 'href': 'restaurants/4.html'},\n",
       " {'name': \"Bojangle's Famous Chicken 'n Biscuits\",\n",
       "  'href': 'restaurants/5.html'},\n",
       " {'name': 'Buffalo Wild Wings', 'href': 'restaurants/6.html'},\n",
       " {'name': 'Burger King', 'href': 'restaurants/7.html'},\n",
       " {'name': \"Captain D's\", 'href': 'restaurants/8.html'},\n",
       " {'name': \"Carl's Jr.\", 'href': 'restaurants/9.html'},\n",
       " {'name': \"Charley's Grilled Subs\", 'href': 'restaurants/10.html'},\n",
       " {'name': 'Chick-fil-A', 'href': 'restaurants/11.html'},\n",
       " {'name': \"Chili's\", 'href': 'restaurants/12.html'},\n",
       " {'name': 'Chipotle Mexican Grill', 'href': 'restaurants/13.html'},\n",
       " {'name': \"Church's\", 'href': 'restaurants/14.html'},\n",
       " {'name': 'Corner Bakery Cafe', 'href': 'restaurants/15.html'},\n",
       " {'name': 'Dairy Queen', 'href': 'restaurants/16.html'},\n",
       " {'name': \"Denny's\", 'href': 'restaurants/17.html'},\n",
       " {'name': 'El Pollo Loco', 'href': 'restaurants/18.html'},\n",
       " {'name': 'FATZ', 'href': 'restaurants/19.html'},\n",
       " {'name': \"Fazoli's\", 'href': 'restaurants/20.html'},\n",
       " {'name': 'Five Guys Burgers and Fries', 'href': 'restaurants/21.html'},\n",
       " {'name': 'Golden Chick', 'href': 'restaurants/22.html'},\n",
       " {'name': \"Hardee's\", 'href': 'restaurants/23.html'},\n",
       " {'name': 'IHOP', 'href': 'restaurants/24.html'},\n",
       " {'name': 'In-N-Out Burger', 'href': 'restaurants/25.html'},\n",
       " {'name': 'Jack in the Box', 'href': 'restaurants/26.html'},\n",
       " {'name': 'Jimmy Johns', 'href': 'restaurants/27.html'},\n",
       " {'name': \"Joe's Crab Shack\", 'href': 'restaurants/28.html'},\n",
       " {'name': 'KFC', 'href': 'restaurants/29.html'},\n",
       " {'name': \"McDonald's\", 'href': 'restaurants/30.html'},\n",
       " {'name': \"O'Charley's\", 'href': 'restaurants/31.html'},\n",
       " {'name': 'Olive Garden', 'href': 'restaurants/32.html'},\n",
       " {'name': 'Outback Steakhouse', 'href': 'restaurants/33.html'},\n",
       " {'name': 'Panda Express', 'href': 'restaurants/34.html'},\n",
       " {'name': 'Panera Bread', 'href': 'restaurants/35.html'},\n",
       " {'name': \"Popeye's\", 'href': 'restaurants/36.html'},\n",
       " {'name': 'Quiznos', 'href': 'restaurants/37.html'},\n",
       " {'name': 'Red Robin Gourmet Burgers', 'href': 'restaurants/38.html'},\n",
       " {'name': \"Romano's Macaroni Grill\", 'href': 'restaurants/39.html'},\n",
       " {'name': 'Ruby Tuesday', 'href': 'restaurants/40.html'},\n",
       " {'name': 'Subway', 'href': 'restaurants/41.html'},\n",
       " {'name': 'Taco Bell', 'href': 'restaurants/42.html'},\n",
       " {'name': 'Taco Bueno', 'href': 'restaurants/43.html'},\n",
       " {'name': \"Wendy's\", 'href': 'restaurants/44.html'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the `href`, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&WÂ® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&WÂ® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\tCategory\tCalories\tFat\tCarbs\n",
    "foods = []\n",
    "for restaurant in restaurants:\n",
    "    html = BeautifulSoup(requests.get(url=url+restaurant['href']).content)\n",
    "    menu_items = html.find('tbody')\n",
    "    for item in menu_items('tr'):\n",
    "        food = {}\n",
    "        food['name'] = item('td')[0].text\n",
    "        food['category'] = item('td')[1].text\n",
    "        food['calories'] = item('td')[2].text\n",
    "        food['fat'] = item('td')[3].text\n",
    "        food['carbs'] = item('td')[4].text\n",
    "        food['restaurant'] = restaurant['name']\n",
    "        foods.append(food)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_df = pd.DataFrame(foods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_df.to_csv('foods.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use `pd.read_html`\n",
    "Do the same thing as above, but use `pd.read_html()` to scrape the table from each page instead of BS4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = []\n",
    "for restaurant in restaurants:\n",
    "    food = pd.read_html(url+restaurant['href'])\n",
    "    food[0]['restaurant'] = restaurant['name']\n",
    "    foods.append(food[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_df = pd.concat(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
