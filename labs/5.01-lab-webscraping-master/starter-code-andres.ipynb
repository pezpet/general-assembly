{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping lab\n",
    "\n",
    "Practice your webscraping and parsing skills! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://pages.git.generalassemb.ly/rldaggie/for-scraping/'\n",
    "res = requests.get(url)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and path from the `href`. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'href': 'restaurants/1.html'}, \n",
    "    {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = []\n",
    "rows = soup.find_all('td')\n",
    "for row in rows:\n",
    "    row_dict = {}\n",
    "    row_dict['name'] = row.find('a').text\n",
    "    row_dict['href'] = row.find('a')['href']\n",
    "    restaurants.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'A&W Restaurants', 'href': 'restaurants/1.html'},\n",
       " {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
       " {'name': \"Arby's\", 'href': 'restaurants/3.html'},\n",
       " {'name': 'Atlanta Bread Company', 'href': 'restaurants/4.html'},\n",
       " {'name': \"Bojangle's Famous Chicken 'n Biscuits\",\n",
       "  'href': 'restaurants/5.html'},\n",
       " {'name': 'Buffalo Wild Wings', 'href': 'restaurants/6.html'},\n",
       " {'name': 'Burger King', 'href': 'restaurants/7.html'},\n",
       " {'name': \"Captain D's\", 'href': 'restaurants/8.html'},\n",
       " {'name': \"Carl's Jr.\", 'href': 'restaurants/9.html'},\n",
       " {'name': \"Charley's Grilled Subs\", 'href': 'restaurants/10.html'},\n",
       " {'name': 'Chick-fil-A', 'href': 'restaurants/11.html'},\n",
       " {'name': \"Chili's\", 'href': 'restaurants/12.html'},\n",
       " {'name': 'Chipotle Mexican Grill', 'href': 'restaurants/13.html'},\n",
       " {'name': \"Church's\", 'href': 'restaurants/14.html'},\n",
       " {'name': 'Corner Bakery Cafe', 'href': 'restaurants/15.html'},\n",
       " {'name': 'Dairy Queen', 'href': 'restaurants/16.html'},\n",
       " {'name': \"Denny's\", 'href': 'restaurants/17.html'},\n",
       " {'name': 'El Pollo Loco', 'href': 'restaurants/18.html'},\n",
       " {'name': 'FATZ', 'href': 'restaurants/19.html'},\n",
       " {'name': \"Fazoli's\", 'href': 'restaurants/20.html'},\n",
       " {'name': 'Five Guys Burgers and Fries', 'href': 'restaurants/21.html'},\n",
       " {'name': 'Golden Chick', 'href': 'restaurants/22.html'},\n",
       " {'name': \"Hardee's\", 'href': 'restaurants/23.html'},\n",
       " {'name': 'IHOP', 'href': 'restaurants/24.html'},\n",
       " {'name': 'In-N-Out Burger', 'href': 'restaurants/25.html'},\n",
       " {'name': 'Jack in the Box', 'href': 'restaurants/26.html'},\n",
       " {'name': 'Jimmy Johns', 'href': 'restaurants/27.html'},\n",
       " {'name': \"Joe's Crab Shack\", 'href': 'restaurants/28.html'},\n",
       " {'name': 'KFC', 'href': 'restaurants/29.html'},\n",
       " {'name': \"McDonald's\", 'href': 'restaurants/30.html'},\n",
       " {'name': \"O'Charley's\", 'href': 'restaurants/31.html'},\n",
       " {'name': 'Olive Garden', 'href': 'restaurants/32.html'},\n",
       " {'name': 'Outback Steakhouse', 'href': 'restaurants/33.html'},\n",
       " {'name': 'Panda Express', 'href': 'restaurants/34.html'},\n",
       " {'name': 'Panera Bread', 'href': 'restaurants/35.html'},\n",
       " {'name': \"Popeye's\", 'href': 'restaurants/36.html'},\n",
       " {'name': 'Quiznos', 'href': 'restaurants/37.html'},\n",
       " {'name': 'Red Robin Gourmet Burgers', 'href': 'restaurants/38.html'},\n",
       " {'name': \"Romano's Macaroni Grill\", 'href': 'restaurants/39.html'},\n",
       " {'name': 'Ruby Tuesday', 'href': 'restaurants/40.html'},\n",
       " {'name': 'Subway', 'href': 'restaurants/41.html'},\n",
       " {'name': 'Taco Bell', 'href': 'restaurants/42.html'},\n",
       " {'name': 'Taco Bueno', 'href': 'restaurants/43.html'},\n",
       " {'name': \"Wendy's\", 'href': 'restaurants/44.html'}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the `href`, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&WÂ® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&WÂ® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\tCategory\tCalories\tFat\tCarbs\n",
    "foods = []\n",
    "for restaurant in restaurants:\n",
    "    html = BeautifulSoup(requests.get(url=url+restaurant['href']).content)\n",
    "    menu_items = html.find('tbody')\n",
    "    for item in menu_items('tr'):\n",
    "        food = {}\n",
    "        food['name'] = item('td')[0].text\n",
    "        food['category'] = item('td')[1].text\n",
    "        food['calories'] = item('td')[2].text\n",
    "        food['fat'] = item('td')[3].text\n",
    "        food['carbs'] = item('td')[4].text\n",
    "        food['restaurant'] = restaurant['name']\n",
    "        foods.append(food)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5131"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_df = pd.DataFrame(foods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_df.to_csv('foods.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use `pd.read_html`\n",
    "Do the same thing as above, but use `pd.read_html()` to scrape the table from each page instead of BS4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name\tCategory\tCalories\tFat\tCarbs\n",
    "foods = []\n",
    "for restaurant in restaurants:\n",
    "    food = pd.read_html(url+restaurant['href'])\n",
    "    foods.append(food)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p1/89v2vyg94jd0blrmm4w41wr00000gn/T/ipykernel_6160/163657905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 )\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "pd.concat(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
